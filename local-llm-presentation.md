# Running LLM on local machine

presentation by Jens for Barcamp 2024

## Why running it locally?

* don't pay for service
* owning a gaming computer
* developing / customizing without paying

## What base models are available

* [Llama3 by Facebook](https://llama.meta.com/llama3/)
* [Gemma by Google](https://ai.google.dev/gemma)
* [Phi3 by Microsoft](https://azure.microsoft.com/en-us/products/phi-3)
* [Mistral](https://mistral.ai/)

## What do I need?

### Software

based on [Llama.cpp](https://github.com/ggerganov/llama.cpp):

* [ollama](https://ollama.com/)
* [LM Studio](https://lmstudio.ai/) (not free for commercial use!)
* [Continue VS Code plugin](https://marketplace.visualstudio.com/items?itemName=Continue.continue)

### Hardware

* modern cpu
* or modern gpu

## What is quantization?


## Use cases

* local chat bot
* code assistant
* Retrieval-augmented generation (RAG)
* home assistant integration (voice chat!)
